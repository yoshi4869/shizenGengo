{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shizenGengo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPpBcxgQIFpe9nM/q1HJeW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoshi4869/shizenGengo/blob/main/shizenGengo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp9vyJ4OOPLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56a2913-eed4-4182-faf8-c30345ad7fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: janome==0.4.1 in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "肉\t名詞,一般,*,*,*,*,肉,ニク,ニク\n",
            "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
            "食べよ\t動詞,自立,*,*,一段,未然ウ接続,食べる,タベヨ,タベヨ\n",
            "う\t助動詞,*,*,*,不変化型,基本形,う,ウ,ウ\n",
            "！\t記号,一般,*,*,*,*,！,！,！\n"
          ]
        }
      ],
      "source": [
        "!pip install janome==0.4.1\n",
        "from janome.tokenizer import Tokenizer\n",
        "#Tokenizerインスタンスの生成\n",
        "tokenizer = Tokenizer()\n",
        "#形態素解析の実施\n",
        "tokens = tokenizer.tokenize(\"肉を食べよう！\")\n",
        "#解析結果の出力：複数の結果が入っておりループ処理で順番に出す\n",
        "for token in tokens:\n",
        "  print(token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome==0.4.1\n",
        "from janome.tokenizer import Tokenizer\n",
        "#Tokenizerインスタンスの生成\n",
        "tokenizer = Tokenizer()\n",
        "#形態素解析の実施\n",
        "tokens = tokenizer.tokenize(\"俺は人間をやめるぞジョジョー！！\")\n",
        "#tokens = tokenizer.tokenize(\"肉を食べよう！\")\n",
        "\n",
        "#解析結果の出力：複数の結果が入っておりループ処理で順番に出す\n",
        "for token in tokens:\n",
        "  print(token)\n",
        "  #print(token.surface)\n",
        "  print(token.reading)\n",
        "  print(token.base_form)\n",
        "  print(token.part_of_speech)\n",
        "  print(token.part_of_speech.split(','))\n",
        "  print(token.part_of_speech.split('.')[0])\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXSrZSI7kX4k",
        "outputId": "7329f3c0-46fd-475e-c213-b96f5d7437b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: janome==0.4.1 in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "俺\t名詞,代名詞,一般,*,*,*,俺,オレ,オレ\n",
            "オレ\n",
            "俺\n",
            "名詞,代名詞,一般,*\n",
            "['名詞', '代名詞', '一般', '*']\n",
            "名詞,代名詞,一般,*\n",
            "-----\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "ハ\n",
            "は\n",
            "助詞,係助詞,*,*\n",
            "['助詞', '係助詞', '*', '*']\n",
            "助詞,係助詞,*,*\n",
            "-----\n",
            "人間\t名詞,一般,*,*,*,*,人間,ニンゲン,ニンゲン\n",
            "ニンゲン\n",
            "人間\n",
            "名詞,一般,*,*\n",
            "['名詞', '一般', '*', '*']\n",
            "名詞,一般,*,*\n",
            "-----\n",
            "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
            "ヲ\n",
            "を\n",
            "助詞,格助詞,一般,*\n",
            "['助詞', '格助詞', '一般', '*']\n",
            "助詞,格助詞,一般,*\n",
            "-----\n",
            "やめる\t動詞,自立,*,*,一段,基本形,やめる,ヤメル,ヤメル\n",
            "ヤメル\n",
            "やめる\n",
            "動詞,自立,*,*\n",
            "['動詞', '自立', '*', '*']\n",
            "動詞,自立,*,*\n",
            "-----\n",
            "ぞ\t助詞,終助詞,*,*,*,*,ぞ,ゾ,ゾ\n",
            "ゾ\n",
            "ぞ\n",
            "助詞,終助詞,*,*\n",
            "['助詞', '終助詞', '*', '*']\n",
            "助詞,終助詞,*,*\n",
            "-----\n",
            "ジョジョー\t名詞,一般,*,*,*,*,ジョジョー,*,*\n",
            "*\n",
            "ジョジョー\n",
            "名詞,一般,*,*\n",
            "['名詞', '一般', '*', '*']\n",
            "名詞,一般,*,*\n",
            "-----\n",
            "！\t記号,一般,*,*,*,*,！,！,！\n",
            "！\n",
            "！\n",
            "記号,一般,*,*\n",
            "['記号', '一般', '*', '*']\n",
            "記号,一般,*,*\n",
            "-----\n",
            "！\t記号,一般,*,*,*,*,！,！,！\n",
            "！\n",
            "！\n",
            "記号,一般,*,*\n",
            "['記号', '一般', '*', '*']\n",
            "記号,一般,*,*\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome==0.4.1\n",
        "from janome.tokenizer import Tokenizer\n",
        "#Tokenizerインスタンスの生成\n",
        "tokenizer = Tokenizer()\n",
        "#形態素解析の実施\n",
        "tokens = tokenizer.tokenize(\"これでみんなで原始人。肉を食べよう！\")\n",
        "#tokens = tokenizer.tokenize(\"肉を食べよう！\")\n",
        "\n",
        "def token2gensigo(input_token):\n",
        "  if input_token.part_of_speech.split(',')[0] == \"助詞\":\n",
        "    return \"\"\n",
        "  else:\n",
        "    return input_token.reading\n",
        "\n",
        "result_str = \"\"\n",
        "for token in tokens:\n",
        "  result_str += token2gensigo(token) + \" \"\n",
        "\n",
        "print(result_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afRbdAWSlxKa",
        "outputId": "4e384436-66a0-4376-cb0f-933bd4c66b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: janome==0.4.1 in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "コレ  ミンナ  ゲンシ ジン 。 ニク  タベヨ ウ ！ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install janome==0.4.1\n",
        "from janome.tokenizer import Tokenizer\n",
        "#Tokenizerインスタンスの生成\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "def nihongo2genshigo(input_str):\n",
        "# 形態素解析の実施\n",
        "  tokens = tokenizer.tokenize(input_str)\n",
        "  result_str = \"\"\n",
        "  for token in tokens:\n",
        "    result_str += token2gensigo(token) + \" \"\n",
        "  return result_str\n",
        "\n",
        "# tokenが助詞の場合は空文字列、それ以外はヨミガナを返す関数\n",
        "def token2gensigo(input_token):\n",
        "  if input_token.part_of_speech.split(',')[0] == \"助詞\":\n",
        "    return \"\"\n",
        "  else:\n",
        "    return input_token.reading\n",
        "\n",
        "print(nihongo2genshigo(\"これで今日からみんな原始人になる\"))\n",
        "print(nihongo2genshigo(\"人民の人民による人民のための政治\"))\n",
        "print(nihongo2genshigo(\"大いなる力には大いなる責任が伴う\"))"
      ],
      "metadata": {
        "id": "B6yimxaam4n5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c966ce4-a18d-4aed-a8b1-2374fc0818f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "コレ  キョウ  ミンナ ゲンシ ジン  ナル \n",
            "ジンミン  ジンミン  ジンミン  タメ  セイジ \n",
            "オオイナル チカラ   オオイナル セキニン  トモナウ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = input(\"文字を入力してみよう\")\n",
        "print(2 * x) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9mzAr7fDJ0r",
        "outputId": "c37bf7b6-fecd-4669-c933-bf54cf738a07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文字を入力してみよう44\n",
            "4444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#文字入力してゲンシ語にする\n",
        "#!pip install janome==0.4.1\n",
        "from janome.tokenizer import Tokenizer\n",
        "#Tokenizerインスタンスの生成\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "def nihongo2genshigo(input_str):\n",
        "# 形態素解析の実施\n",
        "  tokens = tokenizer.tokenize(input_str)\n",
        "  result_str = \"\"\n",
        "  for token in tokens:\n",
        "    result_str += token2gensigo(token) + \" \"\n",
        "  return result_str\n",
        "\n",
        "# tokenが助詞の場合は空文字列、それ以外はヨミガナを返す関数\n",
        "def token2gensigo(input_token):\n",
        "  if input_token.part_of_speech.split(',')[0] == \"助詞\":\n",
        "    return \"\"\n",
        "  else:\n",
        "    return input_token.reading\n",
        "# 文字を打って入力する\n",
        "input_str = input(\"モジ ニュウリョク スル\\n >>\")\n",
        "print(nihongo2genshigo(input_str))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEj-jQtiEVL3",
        "outputId": "0488b5e0-b6e8-4cf0-96b7-994dc4c8f72d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モジ ニュウリョク スル\n",
            " >>俺は人間をやめるぞ\n",
            "オレ  ニンゲン  ヤメル  \n"
          ]
        }
      ]
    }
  ]
}